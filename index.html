<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ali  Kuwajerwala</title>
    <meta name="author" content="Ali  Kuwajerwala" />
    <meta name="description" content="Ali's website, with links and everything. 
" />
    <meta name="keywords" content="ali, kuwajerwala, robotics, engineer, computer, science, artificial, intelligence, academic-website, portfolio-website" />

    <!-- OpenGraph -->
    <meta property="og:site_name" content="Ali  Kuwajerwala" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Ali  Kuwajerwala | about" />
    <meta property="og:url" content="https://alik-git.github.io/" />
    <meta property="og:description" content="Ali's website, with links and everything. 
" />
    <meta property="og:image" content="/assets/img/prof_pic.jpg" />
    <meta property="og:locale" content="en" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="about" />
    <meta name="twitter:description" content="Ali's website, with links and everything. 
" />
    <meta name="twitter:image" content="/assets/img/prof_pic.jpg" />
    <meta name="twitter:site" content="@alihkw_" />
    <meta name="twitter:creator" content="@alihkw_" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- SEE HERE  -->
    <!-- Ali's Fonts Start -->
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Tangerine|Inconsolata|Droid+Sans|Marriweather|Exo+2|Noto+Sans|Arvo">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@100;300;400;500;700;900&display=swap" rel="stylesheet">
    <!-- Ali's Fonts End -->

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🤖</text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://alik-git.github.io/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/podcast/">podcast</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Ali</span>  Kuwajerwala
          </h1>
          <p class="desc">Master's Candidate @ UdeM &amp; Mila (Robotics and AI)</p>
        </header>

        <article>
          <div class="profile float-right">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid z-dept-1 rounded" src="/assets/img/prof_pic.jpg" alt="prof_pic.jpg">

  </picture>

</figure>
<!-- SEE HERE 
            ALI ADDING ANOTHER PICTURE  --><figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/prof_pic_long-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/prof_pic_long-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/prof_pic_long-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid z-dept-1 rounded" src="/assets/img/prof_pic_long.jpg">

  </picture>

</figure>
<!-- END OF ALI'S CHANGES  -->
            <div class="address">
              
            </div>
          </div>

          <div class="clearfix">
            <p>Hi there! 😃</p>

<p>My name is Ali and I’m a soon-to-be-graduating master’s student at <a href="https://mila.quebec/" target="_blank" rel="noopener noreferrer">Mila</a> &amp; the <a href="https://diro.umontreal.ca/accueil/" target="_blank" rel="noopener noreferrer">University of Montréal</a>, supervised by <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Prof. Liam Paull</a> at the <a href="http://montrealrobotics.ca/" target="_blank" rel="noopener noreferrer">Montreal Robotics and Embodied AI Lab</a>. I’m quite passionate about AI and robotics research, and engaged in the quest to understand and simulate intelligence. The <a href="https://concept-fusion.github.io/" target="_blank" rel="noopener noreferrer">ConceptFusion</a> and <a href="https://concept-graphs.github.io/" target="_blank" rel="noopener noreferrer">ConceptGraphs</a> papers which I worked on during my masters are small steps towards this goal.</p>

<p><strong>I am currently actively seeking out work opportunities for both full-time roles and internships. I’m open to relocation and non-robotics roles, what matters to me is working on important problems! You can take a look at my resume <a href="/cv.pdf">here</a>. If you’d like to get in touch, please don’t hesitate to email me at alihusein.kuwajerwala@x, x=umontreal.ca, or on Twitter/LinkedIn!</strong></p>

<p>Previously, I interned at the <a href="https://www.amazon.jobs/en/teams/alexa-ai" target="_blank" rel="noopener noreferrer">Alexa AI</a> team at <a href="https://www.aboutamazon.com/news/amazon-ai" target="_blank" rel="noopener noreferrer">Amazon</a>, where I worked on using large language models for querying databases. In between my undergrad and master’s I was a student researcher at the <a href="https://rvl.cs.toronto.edu/" target="_blank" rel="noopener noreferrer">Robot, Vision and Learning Lab</a> at the <a href="https://www.utoronto.ca/" target="_blank" rel="noopener noreferrer">University of Toronto</a> supervised by <a href="http://www.cs.toronto.edu/~florian/" target="_blank" rel="noopener noreferrer">Prof. Florian Shkurti</a> where I worked on <a href="https://arxiv.org/abs/2110.07668" target="_blank" rel="noopener noreferrer">visual navigation for mobile robots</a>. I’ve also worked in software engineering roles at <a href="https://epson.ca/about-us" target="_blank" rel="noopener noreferrer">Epson</a> and <a href="https://www.liquidanalytics.com/" target="_blank" rel="noopener noreferrer">Liquid Analytics</a>.</p>

<p>My undergrad was at the <a href="https://www.utoronto.ca/" target="_blank" rel="noopener noreferrer">Univeristy of Toronto</a> where I studied CS &amp; Math, and also worked as a Teaching Assistant. During my time there I was co-founder of the <a href="https://utmrobotics.com/" target="_blank" rel="noopener noreferrer">UTM Robotics Club</a>, and a recpient of the <a href="https://www.nserc-crsng.gc.ca/Students-Etudiants/UG-PC/USRA-BRPC_eng.asp" target="_blank" rel="noopener noreferrer">NSERC Undergraduate Student Research Award</a>.</p>

<p>I’ve also worked on a few fun coding projects which you can look at via my <a href="https://github.com/alik-git" target="_blank" rel="noopener noreferrer">github profile</a>.</p>

<p>When I’m not staring at a screen all day, you can find me training jiu-jitsu, listening to audiobooks and podcasts, doodling, and/or chatting with wonderful people on my <a href="/podcast">podcast</a>. ✌️</p>


          </div>

          <!-- News -->          
          <div class="news">
            <h2>news</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row" style="width: 7em;">Sep 28, 2023</th>
                  <td>
                    We released the <a href="https://concept-graphs.github.io/" target="_blank" rel="noopener noreferrer">ConceptGraphs</a> paper to arxiv! 📜

<!-- [cf_arxiv]: https://arxiv.org/abs/2302.07241
[cf_twitter]: https://twitter.com/_krishna_murthy/status/1624948795577454593 -->
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width: 7em;">Apr 5, 2023</th>
                  <td>
                    <a href="https://concept-fusion.github.io/" target="_blank" rel="noopener noreferrer">ConceptFusion</a> was accepted to RSS! 🏛️

<!-- [cf_arxiv]: https://arxiv.org/abs/2302.07241
[cf_twitter]: https://twitter.com/_krishna_murthy/status/1624948795577454593 -->
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width: 7em;">Apr 16, 2022</th>
                  <td>
                    The <a href="https://arxiv.org/abs/2110.07668" target="_blank" rel="noopener noreferrer">Equivariant Representations paper</a> was <a href="https://ieeexplore.ieee.org/document/9811885" target="_blank" rel="noopener noreferrer">accepted</a> to <a href="https://www.icra2022.org/" target="_blank" rel="noopener noreferrer">ICRA 2022</a>! 🎉

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width: 7em;">Mar 31, 2020</th>
                  <td>
                    Received the <a href="https://www.nserc-crsng.gc.ca/students-etudiants/ug-pc/usra-brpc_eng.asp" target="_blank" rel="noopener noreferrer">NSERC Undergraduate Student Research Award</a>! 🤓

<!-- ($4500 in funding) by the [UTM MCS department][]{:target="_blank"}. -->

<!-- [UTM MCS department]: https://www.utm.utoronto.ca/math-cs-stats/ -->
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width: 7em;">Apr 6, 2019</th>
                  <td>
                    Co-founded the <a href="https://www.instagram.com/utm_robotics" target="_blank" rel="noopener noreferrer">UTM Robotics Club</a> as Head of Operations! 🦾

 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          <!-- Selected papers -->
          <div class="publications">
            <h2>selected publications</h2>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="gu2023conceptgraphs" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning</div>
          <!-- Author -->
          <div class="author">Kuwajerwala, Alihusein, Gu, Qiao, Morin, Sacha, Jatavallabhula, Krishna Murthy, Sen, Bipasha, Agarwal, Aditya, Rivera, Corban, Paul, William, Ellis, Kirsty, Chellappa, Rama, Gan, Chuang, Melo, Celso Miguel, Tenenbaum, Joshua B., Torralba, Antonio, Shkurti, Florian, and Paull, Liam
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://concept-graphs.github.io/assets/pdf/2023-ConceptGraphs.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://concept-graphs.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="conceptfusion2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">ConceptFusion: Open-set Multimodal 3D Mapping</div>
          <!-- Author -->
          <div class="author">Jatavallabhula, Krishna Murthy, Kuwajerwala, Alihusein, Gu, Qiao, Omama, Mohd, Chen, Tao, Li, Shuang, Iyer, Ganesh, Saryazdi, Soroush, Keetha, Nikhil, Tewari, Ayush, Tenenbaum, Joshua B., Melo, Celso Miguel, Krishna, Madhava, Paull, Liam, Shkurti, Florian, and Torralba, Antonio
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://concept-fusion.github.io/assets/pdf/2023-ConceptFusion.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://concept-fusion.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts.
  We address both these issues with ConceptFusion, a scene representation that is (1) fundamentally open-set, enabling reasoning beyond a closed set of concepts and (ii) inherently multimodal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today’s foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">conceptfusion2023</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2302.07241}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2302.07241}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{conceptfusion.gif}</span><span class="p">,</span>
  <span class="na">website</span> <span class="p">=</span> <span class="s">{https://concept-fusion.github.io/}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://concept-fusion.github.io/assets/pdf/2023-ConceptFusion.pdf}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, Krishna Murthy and Kuwajerwala, Alihusein and Gu, Qiao and Omama, Mohd and Chen, Tao and Li, Shuang and Iyer, Ganesh and Saryazdi, Soroush and Keetha, Nikhil and Tewari, Ayush and Tenenbaum, Joshua B. and de Melo, Celso Miguel and Krishna, Madhava and Paull, Liam and Shkurti, Florian and Torralba, Antonio}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ConceptFusion: Open-set Multimodal 3D Mapping}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution Share Alike 4.0 International}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="eqreps2022" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Augmenting Imitation Experience via Equivariant Representations</div>
          <!-- Author -->
          <div class="author">Sharma, Dhruv, Kuwajerwala, Alihusein, and Shkurti, Florian
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2022 International Conference on Robotics and Automation (ICRA)</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2110.07668" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://arxiv.org/abs/2110.07668" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The robustness of visual navigation policies trained through imitation often hinges on the augmentation of the training image-action pairs. Traditionally, this has been done by collecting data from multiple cameras, by using standard data augmentations from computer vision, such as adding random noise to each image, or by synthesizing training images. In this paper we show that there is another practical alternative for data augmentation for visual navigation based on extrapolating viewpoint embeddings and actions nearby the ones observed in the training data. Our method makes use of the geometry of the visual navigation problem in 2D and 3D and relies on policies that are functions of equivariant embeddings, as opposed to images. Given an image-action pair from a training navigation dataset, our neural network model predicts the latent representations of images at nearby viewpoints, using the equivariance property, and augments the dataset. We then train a policy on the augmented dataset. Our simulation results indicate that policies trained in this way exhibit reduced cross-track error, and require fewer interventions compared to policies trained using standard augmentation methods. We also show similar results in autonomous visual navigation by a real ground robot along a path of over 500m.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eqreps2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sharma, Dhruv and Kuwajerwala, Alihusein and Shkurti, Florian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Augmenting Imitation Experience via Equivariant Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9383-9389}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{eqreps.png}</span><span class="p">,</span>
  <span class="na">website</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2110.07668}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2110.07668}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA46639.2022.9811885}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2110.07668}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="https://scholar.google.com/citations?user=5Q7kQgIAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/alik-git" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/alihkw" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/alihkw_" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            <a href="https://alik-git.github.io/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a>
            
            </div>

            <div class="contact-note">
              Feel free to reach out via email and check out my github or linkedin profiles.

            </div>
            
          </div>
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Ali  Kuwajerwala. Last updated: December 20, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Mansory & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

